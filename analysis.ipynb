{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38503f0d",
   "metadata": {},
   "source": [
    "# üîç Optimizer Comparison Analysis\n",
    "This notebook visualizes the clean and adversarial accuracy from multiple experiments saved in `results/metrics.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load results CSV\n",
    "df = pd.read_csv('../results/metrics.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc680652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by clean accuracy\n",
    "df_sorted = df.sort_values(by='clean_acc', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=df_sorted, x='optimizer', y='clean_acc')\n",
    "plt.title('Clean Accuracy by Optimizer')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Optimizer')\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by adversarial accuracy\n",
    "df_sorted = df.sort_values(by='adv_acc', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=df_sorted, x='optimizer', y='adv_acc')\n",
    "plt.title('Adversarial Accuracy by Optimizer')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Optimizer')\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c886883",
   "metadata": {},
   "source": [
    "You can add more analysis here, e.g. comparing training loss, learning rate sensitivity, or training time per epoch if logged."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
